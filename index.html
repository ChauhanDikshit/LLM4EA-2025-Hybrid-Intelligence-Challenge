<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM4EA 2025: Hybrid Intelligence Challenge</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;700&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Inter', sans-serif;
      margin: 0;
      background: linear-gradient(to right, #0f2027, #203a43, #2c5364);
      color: white;
      padding: 0 2rem;
    }
    header {
      text-align: center;
      padding: 4rem 0 2rem;
    }
    h1 {
      font-size: 3rem;
      margin-bottom: 0.5rem;
    }
    h2 {
      color: #00e6e6;
      margin-top: 3rem;
    }
    p, ul {
      max-width: 800px;
      margin: auto;
      font-size: 1.1rem;
      line-height: 1.6;
      
    }
    .cta {
      background: #00e6e6;
      color: #000;
      font-weight: bold;
      padding: 1rem 2rem;
      border-radius: 8px;
      text-align: center;
      display: inline-block;
      margin-top: 2rem;
      text-decoration: none;
    }
    form {
      max-width: 600px;
      margin: 2rem auto;
      background: rgba(255, 255, 255, 0.1);
      padding: 2rem;
      border-radius: 10px;
    }
    label, input, textarea {
      display: block;
      width: 100%;
      margin-bottom: 1rem;
    }
    input, textarea {
      padding: 0.5rem;
      border-radius: 5px;
      border: none;
    }
    input[type="submit"] {
      background-color: #00e6e6;
      color: black;
      font-weight: bold;
      cursor: pointer;
    }
    footer {
      text-align: center;
      font-size: 0.9rem;
      padding: 2rem 0;
      margin-top: 4rem;
      color: #ccc;
    }
    .collapsible {
      background-color: #1890ff;
      color: white;
      cursor: pointer;
      padding: 1rem;
      width: 100%;
      border: none;
      text-align: left;
      outline: none;
      font-size: 1.2rem;
      border-radius: 5px;
      margin-bottom: 0.5rem;
    }
    .collapsible:hover {
      background-color: #007acc;
    }
    .content {
      padding: 0 1rem;
      display: none;
      overflow: hidden;
      background-color: rgba(255, 255, 255, 0.05);
      border-radius: 5px;
    }
    .highlight-tracks {
      background-color: #004d66;
      padding: 1rem;
      border-left: 6px solid #00e6e6;
      border-radius: 5px;
      margin-bottom: 1rem;
    }
    .highlight-paragraph {
      padding: 1rem;
      border-left: 4px solid #00e6e6;
      border-radius: 6px;
      margin-top: 1rem;
      width: 100%;
      box-sizing: border-box;
      text-align: justify;
    }
    .highlight-paragraph p {
      max-width: 100%;
      margin-left: 0;
      margin-right: 0;
      text-align: justify;
    }
    .track-description-box {
      background-color: rgba(255, 255, 255, 0.05);
      padding: 1rem;
      border-radius: 0.5rem;
      margin: 1rem 0;
      text-align: justify;
    }
.about-title {
      text-align: center;
    }
  </style>
</head>
<body>
  <header>
    <h1>LLM4EA 2025</h1>
    <h2>Hybrid Intelligence Challenge â€” Evolving with Language</h2>
    <p>Join the global competition to shape the future of optimization by combining the strengths of Large Language Models and Evolutionary Algorithms. </p>
    <a class="cta" href="#register">Register Now</a>
  </header>

  <section>
    <h2 class="about-title">Overview and Theme</h2>
    <div class="highlight-paragraph">
      <p>The LLM4EA 2025 Challenge is a pioneering competition that seeks to explore and define the future of hybrid algorithm design by uniting two powerful paradigms: Large Language Models (LLMs) and Evolutionary Algorithms (EAs). As both fields have evolved rapidly, with EAs demonstrating robust problem-solving across a wide range of domains, and LLMs revolutionizing reasoning, synthesis, and knowledge representation, this challenge aims to harness their synergy to unlock new frontiers in intelligent optimization.</p>
      <p style="text-indent: 2em;">Traditional EA design relies heavily on expert-driven heuristics, trial-and-error parameter tuning, and handcrafted operators. However, these design processes can be abstract, time-consuming, and difficult to generalize across problems. LLMs offer a transformative advantage by introducing natural language reasoning, code generation, prompt engineering, and adaptive decision-making into the optimization workflow. When effectively integrated, LLMs can act as a co-pilot, designer, or even a dynamic component of an EA system.</p>
      <p style="text-indent: 2em;">This challenge embraces the human-in-the-loop and AI-in-the-loop paradigms, inviting researchers to rethink how optimization tools are developed. It opens the door to LLM-guided metaheuristics that can self-adapt, generate operators, respond to problem features, or even evolve their own strategies over time. Participants are encouraged to innovate not only through performance but through explainable and generalizable designs that highlight the potential of intelligent hybrid systems.</p>
      <p style="text-indent: 2em;">The competition features two complementary tracks. Track A emphasizes the creative and conceptual side, how LLMs can drive or support the design of novel EAs, with a focus on methodology and explainability over performance. Track B tests the practical capabilities of such developed hybrid innovation systems by applying them to established benchmark functions from the CEC 2017 suite, evaluating their real-world efficiency, robustness, and competitiveness.</p>
      <p style="text-indent: 2em;">In essence, LLM4EA 2025 is more than a competition, it is a collaborative experiment in building AI systems that can design better AI systems, setting the stage for a new era of optimization research powered by machine reasoning, language understanding, and evolutionary intelligence.</p>
    </div>
  </section>

  <section>
    <h2>Objectives</h2>
    <ul>
      <li>Enable hybrid intelligence by integrating LLMs into EA design, promoting adaptive, generative, and explainable optimization systems.</li>
      <li>Inspire novel EA designs through LLM-assisted reasoning, code generation, and problem understanding.</li>
      <li>Promote both creativity and empirical performance, with Track A focusing on design innovation and Track B on benchmark evaluation.</li>
      <li>Build a collaborative research community around AI-in-the-loop and human-in-the-loop optimization strategies.</li>
</ul>
<section>

  <section>
  <h2>Scope & Tracks</h2>
  <ul>
    <h3>Track A: LLM-Centric Hybrid Algorithm Design (Poster Presentation Track)</h3>
    <ul>
      <li>Participants submit an algorithm or conceptual workflow where an LLM contributes significantly to design, strategy generation, reasoning, or adaptive components.</li>
      <li>Evaluation through poster presentations.</li>
      <li>Participants explain novelty, the design process, and how LLMs are used in designing EA.</li>
    </ul>

    <div class="track-description-box">
      <p>
        The aim is to encourage creativity and advancement in the design of intelligent EAs where LLMs play an integral, explanatory, or generative role in the algorithmic workflow. This track invites participants to explore how LLMs can be used to rethink or co-create EAs in novel and intelligent ways. Submissions should showcase how LLMs assist in the design, adaptation, or generation of EA components and explain the rationale behind their design choices.
      </p>
      <p style="text-indent: 2em;">
        This track emphasizes conceptual novelty, technical creativity, and explainability over raw performance. It's intended for researchers who wish to demonstrate original thinking, frameworks, or architectures, even if they are in early-stage development or lack competitive benchmarking results.
      </p>
    </div>

    <h3>Track B: LLM-Proposed or LLM-Refined Algorithm on CEC 2017 (Benchmark Performance Track)</h3>
    <ul>
      <li>Participants use LLMs to propose or refine EAs and test them on CEC 2017 bound-constrained optimization problems.</li>
      <li>Emphasis is on empirical performance while ensuring a clear role of the LLM in algorithm design or adaptation.</li>
      <li>Evaluation through benchmark performance and reproducibility.</li>
      <li>LLM involvement must be documented and justified.</li>
    </ul>
<div class="track-description-box">
      <p>
        The aim is to evaluate the practical performance and competitive advantage of LLM-assisted EAs by applying them to standardized, real-parameter optimization benchmarks. This track focuses on advancing algorithmic effectiveness through intelligent, LLM-integrated design and decision-making processes.</p>
<p> Participants must demonstrate that the LLM meaningfully contributes to the EA's development, such as through parameter control, variation operator tuning, restart strategies, or diversity mechanisms, and quantify its impact on optimization performance.</p>
     <p style="text-indent: 2em;">
    Submitted algorithms will be tested by the participants themselves on the 30 test problems of the CEC 2017 Bound-Constrained Optimization Suite, a rigorous benchmark set comprising multimodal, hybrid, and composition functions, under 30 dimensions (30D). Performance should be evaluated based on the given evaluation criteria in <a href="https://github.com/P-N-Suganthan/CEC2017-BoundContrained/blob/master/Definitions%20of%20%20CEC2017%20benchmark%20suite%20final%20version%20updated.pdf" target="_blank" style="color: red;"><strong>the CEC 2017 test report</strong></a>, and the results should be compared with the top three tiers' winners (<a href="https://github.com/P-N-Suganthan/2024-CEC/tree/main" target="_blank" style="color: red;"><strong>L-SRDE</strong></a>, <a href="https://github.com/P-N-Suganthan/CEC2017-BoundContrained/tree/master" target="_blank" style="color: red;"><strong>EBOwithCMR</strong></a>, and <a href="https://github.com/P-N-Suganthan/2024-CEC/tree/main" target="_blank" style="color: red;"><strong>RDE</strong></a>). Submissions must include a transparent report documenting the role and contributions of the LLM.
</p>
    </div>

  </ul>
</section>


  <section>
    <h2>Rules & Guidelines</h2>
    <ul>
      <li>Each team may submit one entry per track.</li>
      <li>Submissions must include code, a short report (max 4 pages), and performance logs.</li>
      <li>LLMs must play an active role in the solution pipeline (e.g., operator selection, prompt generation, evaluation, etc.).</li>
      <li>Plagiarism or rule violations may lead to disqualification.</li>
    </ul>
  </section>

   <section>
    <h2>Evaluation Criteria</h2>
    <ul>
      <li>Details are presented in <a href="https://github.com/P-N-Suganthan/CEC2017-BoundContrained/blob/master/Definitions%20of%20%20CEC2017%20benchmark%20suite%20final%20version%20updated.pdf" target="_blank" style="color: red;"><strong>the CEC 2017 test report</strong></a>.</li>
    </ul>
  </section>

  <section>
  <h2>Submission Requirements</h2>
 <ul>
  <h3>For Track A</h3>
 
    <li>A 4-page technical report detailing:
      <ul>
        <li>The proposed EA and details on LLM integration</li>
        <li>Conceptual architecture and motivation</li>
        <li>Preliminary results on the CEC 2017 benchmark</li>
        <li>Explainability features or how the LLM output is interpreted</li>
      </ul>
    </li>
    <li>Reproducible code</li>
    <li>Prompts or training data (if custom or fine-tuned LLMs are used)</li>
  

  <h3>For Track B</h3>

    <li>A 4-page technical report that includes:
      <ul>
        <li>The proposed EA and details on LLM integration</li>
        <li>Conceptual architecture and motivation</li>
        <li>System overview and LLM integration details</li>
        <li>Benchmark setup and test protocol</li>
        <li>Results tables and comparisons with L-SRDE, RDE, 1st and 2nd winners of the CEC 2024 competition, and EBOwithCMR, winner of the CEC 2017 competition.</li>
      </ul>
    </li>
    <li>Complete source code with README</li>
    <li>Benchmark result files (CSV format)</li>
  </ul>
</section>


  <section>
    <h2>Timeline</h2>
    <ul>
      <li>Launch: May 19, 2025</li>
      <li>Registration Deadline: June 5, 2025</li>
      <li>Submission Deadline: June 10, 2025</li>
      <li>Winners Announced: June 20, 2025</li>
    </ul>
  </section>

  <section>
    <h2>Prizes</h2>
    <ul>
      <li> First three winners will receive certificates.</li>
    </ul>
  </section>

<!--   <section>
    <h2>Registration</h2>
    <p>To participate in the LLM4EA 2025 Challenge, first complete the workshop registration at the link below. Registering for the workshop automatically qualifies you to participate in the competition.</p>
    <p><strong><a class="cta" href="https://www.nitj.ac.in/events_registration/workshop_sseaml_2025/login" target="_blank">Workshop Registration Portal</a></strong></p>
    <p>After registering at the above link, please complete the form below using your registration details.</p>
  </section>

  <section id="register">
    <h2>Complete Registration for Competition</h2>
    <p>
      <strong><a class="cta" href="https://docs.google.com/forms/d/e/1FAIpQLSd2oDC89jcNNe1_s6Q5x-j7E20MZvqTojaAOpM8DRo6IO9bbQ/viewform" target="_blank">Competition Registration Portal</a></strong>
    </p>
  </section> -->

  <section id="register">
  <h2>Registration for Competition</h2>
  <p>
    <strong><a class="cta" href="https://docs.google.com/forms/d/e/1FAIpQLSd2oDC89jcNNe1_s6Q5x-j7E20MZvqTojaAOpM8DRo6IO9bbQ/viewform" target="_blank">Competition Registration Portal</a></strong>
  </p>
</section>

  <section>
    <h2>Leaderboard (Coming Soon)</h2>
    <p>The top-ranked teams and their scores will be displayed here after the evaluation phase.</p>
  </section>

  <section>
    <h2>Participant Login (Coming Soon)</h2>
   <p>Send results via <a><strong>evoml@nitj.ac.in</strong></a></p>
  </section>

  <section>
  <h2>Organizer & Contact Info</h2>
    <ul>
    <li>
      <a href="https://sites.google.com/view/dikshitchauhan/home" target="_blank">Dr. Dikshit Chauhan</a>, National University of Singapore
    </li>
    <li>
      <a href="https://scholar.google.com/citations?user=WFpIByoAAAAJ&hl=en&oi=ao" target="_blank">Dr. Anupam Trivedi</a>, National University of Singapore
    </li>
    <li>
      <a href="https://scholar.google.com/citations?user=qTB0f40AAAAJ&hl=en&oi=ao" target="_blank">Dr. Bapi Dutta</a>, Universidad de JaÃ©n, Spain
    </li>
    <li>
      <a href="https://departments.nitj.ac.in/dept/ma/Faculty/6430446b38bff038a7808524" target="_blank">Dr. Anupam Yadav</a>, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar
    </li>
  </ul>
<br>
  <p style="text-align: center;">
    <strong>Department of Mathematics and Computing, Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab, India.</strong>
  </p> 
 <p style="text-align: center;">For inquiries, contact: <a href="mailto:evoml@nitj.ac.in"><strong>evoml@nitj.ac.in</strong></a></p>
</section>


  <footer>
    &copy; 2025 LLM4EA Challenge. All rights reserved.
  </footer>

  <script>
    document.querySelectorAll(".collapsible").forEach(button => {
      button.addEventListener("click", () => {
        button.classList.toggle("active");
        const content = button.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    });
  </script>
</body>
</html>
